{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "FinalProjectstarter.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg8Uz9O7iY8R"
      },
      "source": [
        "# Article recommanding system introduction:\n",
        "Article recommanding system is a system makes use natural language processing and unsupervised k-nearest neighbour to recommand k most relevant articles by papers' title and abstract. It does the task by vectorzing the provided paper's text and using knn to match the \"closest\" to the provided paper in the database. In addition, the system provides filter function which accurates to the month to filter out the paper earlier than the specified year and month.\n",
        "\n",
        "# Insight Deriving:\n",
        "Inside the OVID-19 Open Research Dataset (CORD-19), there are over \n",
        "The system helps the users to find relevant papers of a provided paper among all papers in the database by simply providing the title and the abstract of the provided paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilfwLJoQMjR5"
      },
      "source": [
        "# Install and import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVSvkvnfJeLZ"
      },
      "source": [
        "!pip install tqdm_notebook > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6plclfo7rc3"
      },
      "source": [
        "!pip install scispacy > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI-cky_p7sXU"
      },
      "source": [
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXMxwxjF8VaS"
      },
      "source": [
        "#Import Libraries\n",
        "\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import html\n",
        "import re\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import scispacy\n",
        "import spacy\n",
        "import en_core_sci_lg\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTMUNt4v888s"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google.colab import drive\n",
        "  from tqdm.notebook import tqdm_notebook as tqdm\n",
        "  tqdm.pandas()\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRr-0r9bizjh"
      },
      "source": [
        "**Please modify the following path to run in your local machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o6oNNsbQh71"
      },
      "source": [
        "base_path = \"/content/drive/MyDrive/Colab Notebooks/data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5-oTBvUPuxd"
      },
      "source": [
        "# Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xglOimN8VaT"
      },
      "source": [
        "df = pd.read_csv(base_path+\"metadata.csv\", low_memory=False)\n",
        "df.info(verbose=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ98z19I8VaT"
      },
      "source": [
        "# load the meta data from the CSV file and extract out fields that will be useful to our application\n",
        "\n",
        "df = df[['title','abstract','publish_time', 'cord_uid', 'who_covidence_id']]\n",
        "df = df.drop_duplicates(subset='abstract', keep=\"first\")\n",
        "df=df.dropna()\n",
        "df[\"abstract\"] = df[\"abstract\"].str.lower()\n",
        "df[\"title\"] = df[\"title\"].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uElUPWp1KWO6"
      },
      "source": [
        "Only choose the articles related to COVID-19 for this project. I assume that papers with a who_covidence_id are related to COVID-19."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9OSY_MEoRxR"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH3h_dDHorbo"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HRbi8xBLENo"
      },
      "source": [
        "We use scispaCy for preprocessing, which is a Python package containing spaCy models for processing scientific text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJoH2RZK7EeC"
      },
      "source": [
        "nlp = en_core_sci_lg.load(disable=[\"parser\", \"ner\"])\n",
        "nlp.max_length = 2000000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhKY3VsQ9KjT"
      },
      "source": [
        "def clean_text(sentence):\n",
        "  sentence = html.unescape(sentence) # replaces HTML charachter codes with ASCII equivalent\n",
        "  sentence = re.sub(r\"http\\S+\", \"\", sentence) # removes URL links\n",
        "  wordsList = []\n",
        "  for word in nlp(sentence):\n",
        "    if not (word.is_stop or word.is_space or word.like_num or word.is_punct or len(word)<2):\n",
        "      wordsList.append(word.lemma_)\n",
        "  return \" \".join(wordsList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhgmGC9YL98t"
      },
      "source": [
        "Add customize stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNLnIAqG-bXQ"
      },
      "source": [
        "customize_stop_words = [\n",
        "    'doi', 'preprint', 'copyright', 'https', 'author', 'figure', 'table',\n",
        "    'rights', 'reserved', 'permission', 'use', 'biorxiv', 'medrxiv', \n",
        "    'license', 'fig', 'fig.', 'al.', 'PMC', 'CZI', \n",
        "    'br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \\\n",
        "    \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \\\n",
        "    'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", \\\n",
        "    'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', \\\n",
        "    'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', \\\n",
        "    'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', \\\n",
        "    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', \\\n",
        "    'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', \\\n",
        "    'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', \\\n",
        "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', \\\n",
        "    'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", \\\n",
        "    'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", \\\n",
        "    'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \\\n",
        "    \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \\\n",
        "    \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", \\\n",
        "    'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
        "]\n",
        "for word in customize_stop_words:\n",
        "    nlp.vocab[word].is_stop = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS1V4TNbMQVi"
      },
      "source": [
        "Apply the nlp model to title and abstract columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a23j3_3aJQKg"
      },
      "source": [
        "def clean_df(data):\n",
        "  data['cleaned_title'] = data['title'].progress_apply(clean_text)\n",
        "  data['cleaned_abstract'] = data['abstract'].progress_apply(clean_text)\n",
        "clean_df(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7wCbsUCNicd"
      },
      "source": [
        "Convert the publish time to datetime object, and categorize the time into year and month separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hDsRwL6Xy8B"
      },
      "source": [
        "df['publish_time'] = pd.to_datetime(df['publish_time'])\n",
        "def categorize_time(data):\n",
        "  data['publish_year'] = data['publish_time'].dt.year\n",
        "  data['publish_month'] = data['publish_time'].dt.month\n",
        "categorize_time(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E1ZQ3i8OwiQ"
      },
      "source": [
        "Save the preprocessed dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4cXROldvTxF"
      },
      "source": [
        "df.to_csv(base_path + \"preproced_metadata.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdKepwSBXSJw"
      },
      "source": [
        "# Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a46CedQkXfXP"
      },
      "source": [
        "proced_df = pd.read_csv(base_path + \"preproced_metadata.csv\", low_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hed3OlA4LfH3"
      },
      "source": [
        "proced_df = proced_df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdTVi6kaXrAI"
      },
      "source": [
        "proced_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBp92Zr-1OeY"
      },
      "source": [
        "## publish year distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKfzDAGnO7Iq"
      },
      "source": [
        "As we can see, most of the articles articles published in 2020, so filtering the articles by year would not be very helpful. Therefore, we can add month to the filtering condition to help user filter the articles according to the publish time more accurately. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYtxs5KgZAOc"
      },
      "source": [
        "proced_df['publish_year'].plot.hist()\n",
        "plt.xlabel('Publish year')  \n",
        "plt.ylabel('Counts')\n",
        "plt.title('Publish year distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA8li4_aD8Y8"
      },
      "source": [
        "## Plot word frequency for abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrqrIqwX-IJT"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb7SIkSh-Szn"
      },
      "source": [
        "def get_word_freq(data, top_N=20):\n",
        "  preproced_data = data.str.cat(sep=' ')\n",
        "  words = nltk.tokenize.word_tokenize(preproced_data)\n",
        "  words_selected = [i for i in words if len(i) > 2]\n",
        "  words_dist = nltk.FreqDist(words_selected)\n",
        "  words_freq = pd.DataFrame(words_dist.most_common(top_N),\n",
        "                      columns=['Word', 'Frequency'])\n",
        "  return words_freq, words_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcHGtL9BQ0lp"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "def draw_word_cloud(word_dist, title):\n",
        "  wordcloud = WordCloud(width=800, height=500, max_font_size=110).generate_from_frequencies(word_dist)\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "  plt.title(\"Title words cloud\")\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ehTm6AE-03E"
      },
      "source": [
        "abstract_words_freq, abstract_words_dist = get_word_freq(proced_df['cleaned_abstract'], top_N=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v92Ohp6oBQlR"
      },
      "source": [
        "abstract_words_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE3RjMotCmln"
      },
      "source": [
        "abstract_words_freq.plot(x='Word', y='Frequency', kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJtum73bRBBK"
      },
      "source": [
        "draw_word_cloud(abstract_words_dist, \"Abstract word cloud\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFufBmuGEBbI"
      },
      "source": [
        "## Plot word frequency for title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ecnaaUJEHJm"
      },
      "source": [
        "title_words_freq, title_words_dist = get_word_freq(proced_df['cleaned_title'], top_N=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "091WCwe1ETTw"
      },
      "source": [
        "title_words_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuTwI_waEUd8"
      },
      "source": [
        "title_words_freq.plot(x='Word', y='Frequency', kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIxZvbzRQXt2"
      },
      "source": [
        "draw_word_cloud(title_words_dist, \"Title word cloud\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eaTOwONRl-a"
      },
      "source": [
        "## Data visualization summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztU1EPbzRq5F"
      },
      "source": [
        "By visualizing the data, I find out that words directly related Covid-19 appears very frequently in both title and abstract. Therefore, if we want to make distictions and calculate meaningful distances between articles, we have to ignore words that appears most frequent in title and abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkUCPc0oFOn3"
      },
      "source": [
        "# Model selection and fitting to the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDdt3u7eUNaZ"
      },
      "source": [
        "Since there are no easy ways to evaluate an unsupervised model, I only choose a small subset data as test set to test the functionality of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rzadIKWkyeP"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(proced_df, test_size=100, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJX0G9sQh6iC"
      },
      "source": [
        "print(f\"Training set shape: {train_df.shape}\")\n",
        "print(f\"Testing set shape: {test_df.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLPPc8fPUtOD"
      },
      "source": [
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "class NNWrapper(TransformerMixin):\n",
        "  '''\n",
        "  Nearest Neighbour wrapper for pipeline\n",
        "  '''\n",
        "  def __init__(self, knn_args):\n",
        "    self.knn = NearestNeighbors(**knn_args)\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    self.knn.fit(X)\n",
        "\n",
        "  def predict(self, X, **predict_params):\n",
        "    return self.knn.kneighbors(X, predict_params['n_neighbors'], predict_params['return_distance'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uErw2EUrWqt1"
      },
      "source": [
        "I use a simple pipeline to train my model by following steps below:\n",
        "\n",
        "\n",
        "1.   I use a column transformer to process title and abstract independently and I choose TFIDF as my vectorizer since it usually performs better than word frequency in my experience.\n",
        "2.   Then the preprocessed data is fed into the knn model for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dUCN9-RBVqF"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "def train(train_df, title_args, abstract_args, knn_args):\n",
        "  preprocess = ColumnTransformer([\n",
        "                                  ('title', TfidfVectorizer(**title_args), 'cleaned_title'),\n",
        "                                  ('abstract', TfidfVectorizer(**abstract_args), 'cleaned_abstract')\n",
        "                                  ], remainder = 'drop', n_jobs=-1)\n",
        "          \n",
        "  pipe = Pipeline([\n",
        "                  ('preprocess', preprocess),\n",
        "                  ('model', NNWrapper(knn_args))\n",
        "          ],\n",
        "          verbose=True)\n",
        "  pipe.fit(train_df)\n",
        "  return pipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZGPlikQXa9-"
      },
      "source": [
        "In the cell below, I specify the arguments for the models in the pipeline.\n",
        "In particular, note that by setting min_df and max_df, I only take the words that appear more than 10% and less than 90%. By setting min_df, we don't consider the extremely rare words, and the reason for setting max_df has been stated in data visualization part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXxtpvrAT7Qh"
      },
      "source": [
        "MAX_FEATURES_TITLE = 1000\n",
        "MAX_FEATURES_ABSTRACT = 10000\n",
        "title_args = {'min_df': 0.1,  'max_df':0.9,'max_features': MAX_FEATURES_TITLE, 'ngram_range': (1, 3)}\n",
        "abstract_args = {'min_df': 0.1,  'max_df':0.9, 'max_features': MAX_FEATURES_ABSTRACT, 'ngram_range': (1, 4)}\n",
        "knn_args = {'n_neighbors': 3, 'n_jobs': -1}\n",
        "model = train(train_df, title_args, abstract_args, knn_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh42pUpoTMNo"
      },
      "source": [
        "def get_kneighbors(model, X, k=3, return_distance=True):\n",
        "  return model.predict(X, n_neighbors=k, return_distance=return_distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrRQ_FOqnswx"
      },
      "source": [
        "def recommand(model, paper, database, no_earlier=None, k=3):\n",
        "  '''\n",
        "  The function takes an input of a single paper and recommands the k most relevant papers in the database.\n",
        "  no_earlier(str): is a filter to discard the papers strictly earlier than the argument. It should be in the format of year-month(e.g. '2020-08')\n",
        "  '''\n",
        "  no_earlier_dt = pd.to_datetime(no_earlier)\n",
        "  dists, nn = get_kneighbors(model, paper, k)\n",
        "  recommand_papers = database.iloc[nn[0], :].copy()\n",
        "  recommand_papers['distance'] = dists[0]\n",
        "  if no_earlier:\n",
        "    return recommand_papers[recommand_papers['publish_time'] > no_earlier]\n",
        "  return recommand_papers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1zNUrBR86bt"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mISeT_1l9A0e"
      },
      "source": [
        "test_paper = test_df.iloc[[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwmpXC049GPE"
      },
      "source": [
        "test_paper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77F3_vXtysZK"
      },
      "source": [
        "recommand_papers = recommand(model, test_paper, train_df, k=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-rCxi2r7TPg"
      },
      "source": [
        "recommand_papers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wErR-Sq7pQfd"
      },
      "source": [
        "recommand_papers_filtered = recommand(model, test_paper, train_df, k=5, no_earlier='2020-09')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isLq2pQipZD_"
      },
      "source": [
        "recommand_papers_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVPYr03JbDlE"
      },
      "source": [
        "recommand_papers.plot(x='cord_id', y='distance', kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZbLnuqPl_wm"
      },
      "source": [
        "def retrieve_all_recom(model, papers, database, no_earlier=None, k=2):\n",
        "  paper_storer = []\n",
        "\n",
        "  for i in range(0, papers.shape[0]):\n",
        "    orig_paper = papers.iloc[[i]]\n",
        "    recom_papers = recommand(model, orig_paper, database, k=k)\n",
        "    recom_papers['original_paper_title'] = orig_paper['title'].values[0]\n",
        "    paper_storer.append(recom_papers)\n",
        "  all_recommand_papers = pd.concat(paper_storer, axis=0)\n",
        "  if no_earlier:\n",
        "    return all_recommand_papers[all_recommand_papers['publish_time'] > no_earlier]\n",
        "  return all_recommand_papers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEuvSZ_7esNT"
      },
      "source": [
        "all_recommand_papers = retrieve_all_recom(model, test_df, train_df, k=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjdFWPItf2OX"
      },
      "source": [
        "all_recommand_papers"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}